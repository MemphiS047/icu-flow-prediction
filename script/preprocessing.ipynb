{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import dotenv\n",
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dataextraction as db\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the environment variables from the .env file\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = db.connect_to_database()\n",
    "cur = conn.cursor()\n",
    "df = db.get_base_dataset(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of duplicate subjects\n",
    "df['subject_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only first ICU admissions\n",
    "df = df[df['first_icu_stay'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicate subject_id's again\n",
    "df['subject_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate missing mean columns by taking average of minimum and maximum\n",
    "missing_mean_columns = utils.detect_missing_mean_columns(df)\n",
    "df = utils.add_missing_mean_columns(df, missing_mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then aggregate rest of the mean columns by taking average of the subject_id's multiple mean values\n",
    "mean_columns = [col for col in df.columns if '_mean' in col]\n",
    "aggregation_functions = {}\n",
    "for col in mean_columns:\n",
    "    aggregation_functions[col] = 'mean'\n",
    "aggregation_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated dataframe for later merging it with the original dataframe\n",
    "df_aggregated = df.groupby('subject_id').agg(aggregation_functions).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the aggregated dataframe\n",
    "df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of duplicate subject_id's in aggregated dataframe\n",
    "df_aggregated.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the aggregated dataframe with the original dataframe on subject_id by excluding minimum and maximum and mean columns since\n",
    "# df_aggregated includes unique subject_id's mean (aggregated measurements)\n",
    "other_columns = [col for col in df.columns if('_mean' not in col and '_min' not in col and '_max' not in col)]\n",
    "other_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by subject_id and taking the first value of each column\n",
    "df_other_columns = df.groupby('subject_id', as_index=False)[other_columns].first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging it with the rest of the dataframe on subject_id (since we grouped by first we only left with unique\n",
    "# subject_id's measurements with respect to df_aggregated)\n",
    "df = pd.merge(df_aggregated, df_other_columns, on='subject_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out duplicate subject_id's to make sure we have unique subject_id's\n",
    "df['subject_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out glucose_mean column to see if it is aggregated correctly\n",
    "df['glucose_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the record of subject id that equals to 3 \\\n",
    "df[df[\"subject_id\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on column types for possible encoding of the categorical columns and possible\n",
    "# columns that could be removed because it is irrelevant to the prediction\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting the datetime columns\n",
    "for key, val in df.dtypes.to_dict().items():\n",
    "    if('date' in str(val)):\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the detected datetime columns\n",
    "columns_to_remove = ['intime', 'outtime', 'dod', 'admittime', 'dischtime', 'deathtime', 'edregtime', 'edouttime']\n",
    "df = df.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the dataframe size if the columns are dropped correctly\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting columns with the null values that is above 90% of the total dataframe size\n",
    "columns_to_remove = []\n",
    "for key, value in df.isnull().sum().to_dict().items():\n",
    "    if(((value / df.shape[0]) * 100) > 50):\n",
    "        print(key, value)\n",
    "        columns_to_remove.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the detected columns with the null values that is above 90% of the total dataframe size\n",
    "df = df.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out dataframe size if the columns are dropped correctly\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting the categorical columns \n",
    "for key, val in df.dtypes.to_dict().items():\n",
    "    if('object' in str(val)):\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding the gender columns\n",
    "df['gender'] = df['gender'].map({'M': 0, 'F': 1})\n",
    "df['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency mapping diagnosis column since it has too many unique values\n",
    "frequency_mapping = df['diagnosis'].value_counts(normalize=True)\n",
    "df['diagnosis_encoded'] = df['diagnosis'].map(frequency_mapping)\n",
    "df['diagnosis_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For rest of the cateogircal columns we will use one hot encoding\n",
    "categorical_cols = ['marital_status', 'ethnicity_grouped',\n",
    "                    'first_careunit', 'last_careunit', 'admission_type',\n",
    "                    'admission_location', 'discharge_location', 'insurance']\n",
    "df = pd.get_dummies(df, columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rest of the unecessary columns \n",
    "columns_to_remove = ['language', 'religion', 'diagnosis', 'ethnicity', 'dbsource']\n",
    "df = df.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in df.dtypes.to_dict().items():\n",
    "    if('object' in str(val)):\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataset for use in eda later on\n",
    "# df.to_csv(f'{os.getenv(\"ROOT_DIR\")}\\\\data\\\\final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting columns with the null values that is above 90% of the total dataframe size\n",
    "for key, value in df.isnull().sum().to_dict().items():   \n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca_result = pca.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_result = umap_reducer.fit_transform(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c='b', marker='o', label='t-SNE')\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(umap_result[:, 0], umap_result[:, 1], c='r', marker='s', label='UMAP')\n",
    "plt.title('UMAP Visualization')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
