{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import dotenv\n",
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dataextraction as db\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN, SpectralClustering, KMeans\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the environment variables from the .env file\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the PostgreSQL database\n",
      "PostgreSQL version: PostgreSQL 14.0, compiled by Visual C++ build 1914, 64-bit\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "conn = db.connect_to_database()\n",
    "cur = conn.cursor()\n",
    "df = db.get_base_dataset(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11976"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of duplicate subjects\n",
    "df['subject_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only first ICU admissions\n",
    "df = df[df['first_icu_stay'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9321"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check duplicate subject_id's again\n",
    "df['subject_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Aggregate missing mean columns by taking average of minimum and maximum\u001b[39;00m\n\u001b[0;32m      2\u001b[0m missing_mean_columns \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdetect_missing_mean_columns(df)\n\u001b[1;32m----> 3\u001b[0m df \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49madd_missing_mean_columns(df, missing_mean_columns)\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\script\\utils.py:18\u001b[0m, in \u001b[0;36madd_missing_mean_columns\u001b[1;34m(df, missing_mean_cols)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_missing_mean_columns\u001b[39m(df, missing_mean_cols):\n\u001b[0;32m     17\u001b[0m     \u001b[39mfor\u001b[39;00m min_col, mean_col \u001b[39min\u001b[39;00m missing_mean_cols:\n\u001b[1;32m---> 18\u001b[0m         df[mean_col] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: calculate_mean(row, min_col, min_col\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m_min\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m_max\u001b[39;49m\u001b[39m'\u001b[39;49m)), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9412\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9414\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9415\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9416\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9421\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9422\u001b[0m )\n\u001b[1;32m-> 9423\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    676\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 798\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    800\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    801\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:812\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    809\u001b[0m results \u001b[39m=\u001b[39m {}\n\u001b[0;32m    811\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 812\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    813\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m    814\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf(v)\n\u001b[0;32m    815\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    816\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    817\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:920\u001b[0m, in \u001b[0;36mFrameColumnApply.series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    919\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseries_generator\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 920\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues\n\u001b[0;32m    921\u001b[0m     values \u001b[39m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n\u001b[0;32m    922\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(values) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:639\u001b[0m, in \u001b[0;36mFrameApply.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m    638\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalues\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mvalues\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:11360\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  11286\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m  11287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalues\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m  11288\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m  11289\u001b[0m \u001b[39m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  11290\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11358\u001b[0m \u001b[39m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  11359\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 11360\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mas_array()\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1732\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1730\u001b[0m         arr\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1732\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interleave(dtype\u001b[39m=\u001b[39;49mdtype, na_value\u001b[39m=\u001b[39;49mna_value)\n\u001b[0;32m   1733\u001b[0m     \u001b[39m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1734\u001b[0m     \u001b[39m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1736\u001b[0m \u001b[39mif\u001b[39;00m na_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39mno_default:\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1776\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m   1775\u001b[0m     rl \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mmgr_locs\n\u001b[1;32m-> 1776\u001b[0m     arr \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39;49mget_values(dtype)\n\u001b[0;32m   1777\u001b[0m     result[rl\u001b[39m.\u001b[39mindexer] \u001b[39m=\u001b[39m arr\n\u001b[0;32m   1778\u001b[0m     itemmask[rl\u001b[39m.\u001b[39mindexer] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1801\u001b[0m, in \u001b[0;36mEABackedBlock.get_values\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1799\u001b[0m values: ArrayLike \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m   1800\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39m==\u001b[39m _dtype_obj:\n\u001b[1;32m-> 1801\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mastype(\u001b[39mobject\u001b[39;49m)\n\u001b[0;32m   1802\u001b[0m \u001b[39m# TODO(EA2D): reshape not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(values)\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32md:\\workspace\\github\\icu-flow-prediction\\.venv\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:701\u001b[0m, in \u001b[0;36mDatetimeArray.astype\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[39melif\u001b[39;00m is_period_dtype(dtype):\n\u001b[0;32m    700\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_period(freq\u001b[39m=\u001b[39mdtype\u001b[39m.\u001b[39mfreq)\n\u001b[1;32m--> 701\u001b[0m \u001b[39mreturn\u001b[39;00m dtl\u001b[39m.\u001b[39;49mDatetimeLikeArrayMixin\u001b[39m.\u001b[39;49mastype(\u001b[39mself\u001b[39;49m, dtype, copy)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Aggregate missing mean columns by taking average of minimum and maximum\n",
    "missing_mean_columns = utils.detect_missing_mean_columns(df)\n",
    "df = utils.add_missing_mean_columns(df, missing_mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then aggregate rest of the mean columns by taking average of the subject_id's multiple mean values\n",
    "mean_columns = [col for col in df.columns if '_mean' in col]\n",
    "aggregation_functions = {}\n",
    "for col in mean_columns:\n",
    "    aggregation_functions[col] = 'mean'\n",
    "aggregation_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated dataframe for later merging it with the original dataframe\n",
    "df_aggregated = df.groupby('subject_id').agg(aggregation_functions).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the aggregated dataframe\n",
    "df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of duplicate subject_id's in aggregated dataframe\n",
    "df_aggregated.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the aggregated dataframe with the original dataframe on subject_id by excluding minimum and maximum and mean columns since\n",
    "# df_aggregated includes unique subject_id's mean (aggregated measurements)\n",
    "other_columns = [col for col in df.columns if('_mean' not in col and '_min' not in col and '_max' not in col)]\n",
    "other_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by subject_id and taking the first value of each column\n",
    "df_other_columns = df.groupby('subject_id', as_index=False)[other_columns].first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging it with the rest of the dataframe on subject_id (since we grouped by first we only left with unique\n",
    "# subject_id's measurements with respect to df_aggregated)\n",
    "df = pd.merge(df_aggregated, df_other_columns, on='subject_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out duplicate subject_id's to make sure we have unique subject_id's\n",
    "df['subject_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out glucose_mean column to see if it is aggregated correctly\n",
    "df['glucose_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the record of subject id that equals to 3 \\\n",
    "df[df[\"subject_id\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on column types for possible encoding of the categorical columns and possible\n",
    "# columns that could be removed because it is irrelevant to the prediction\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting the datetime columns\n",
    "for key, val in df.dtypes.to_dict().items():\n",
    "    if('date' in str(val)):\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the detected datetime columns\n",
    "columns_to_remove = ['intime', 'outtime', 'dod', 'admittime', 'dischtime', 'deathtime', 'edregtime', 'edouttime']\n",
    "df = df.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the dataframe size if the columns are dropped correctly\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting columns with the null values that is above 90% of the total dataframe size\n",
    "columns_to_remove = []\n",
    "for key, value in df.isnull().sum().to_dict().items():\n",
    "    if(((value / df.shape[0]) * 100) > 50):\n",
    "        print(key, value)\n",
    "        columns_to_remove.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the detected columns with the null values that is above 90% of the total dataframe size\n",
    "df = df.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out dataframe size if the columns are dropped correctly\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting the categorical columns \n",
    "for key, val in df.dtypes.to_dict().items():\n",
    "    if('object' in str(val)):\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding the gender columns\n",
    "df['gender'] = df['gender'].map({'M': 0, 'F': 1})\n",
    "df['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency mapping diagnosis column since it has too many unique values\n",
    "frequency_mapping = df['diagnosis'].value_counts(normalize=True)\n",
    "df['diagnosis_encoded'] = df['diagnosis'].map(frequency_mapping)\n",
    "df['diagnosis_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For rest of the cateogircal columns we will use one hot encoding\n",
    "categorical_cols = ['marital_status', 'ethnicity_grouped',\n",
    "                    'first_careunit', 'last_careunit', 'admission_type',\n",
    "                    'admission_location', 'discharge_location', 'insurance']\n",
    "df = pd.get_dummies(df, columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rest of the unecessary columns \n",
    "columns_to_remove = ['language', 'religion', 'diagnosis', 'ethnicity', 'dbsource']\n",
    "df = df.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in df.dtypes.to_dict().items():\n",
    "    if('object' in str(val)):\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataset for use in eda later on\n",
    "# df.to_csv(f'{os.getenv(\"ROOT_DIR\")}\\\\data\\\\final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting columns with the null values that is above 90% of the total dataframe size\n",
    "for key, value in df.isnull().sum().to_dict().items():   \n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca_result = pca.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_result = umap_reducer.fit_transform(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c='b', marker='o', label='t-SNE')\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(umap_result[:, 0], umap_result[:, 1], c='r', marker='s', label='UMAP')\n",
    "plt.title('UMAP Visualization')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(umap_result)\n",
    "\n",
    "df['GMM_Cluster_UMAP'] = gmm_labels\n",
    "\n",
    "plt.scatter(umap_result[:, 0], umap_result[:, 1], c=gmm_labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.title('GMM Clustering using t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.8\n",
    "min_samples = 5\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan_labels = dbscan.fit_predict(umap_result)\n",
    "\n",
    "df['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "plt.scatter(umap_result[:, 0], umap_result[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.title('DBSCAN Clustering using t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size = 5\n",
    "\n",
    "hdb = hdbscan.HDBSCAN(min_samples=min_cluster_size)\n",
    "hdb_labels = hdb.fit_predict(umap_result)\n",
    "\n",
    "df['HDBSCAN_Cluster'] = hdb_labels\n",
    "\n",
    "plt.scatter(umap_result[:, 0], umap_result[:, 1], c=hdb_labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.title('HDBSCAN Clustering using t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "\n",
    "spectral_clustering = SpectralClustering(n_clusters=n_clusters, eigen_solver='arpack', random_state=42)\n",
    "spectral_labels = spectral_clustering.fit_predict(umap_result)\n",
    "\n",
    "df['Spectral_Cluster'] = spectral_labels\n",
    "\n",
    "plt.scatter(umap_result[:, 0], umap_result[:, 1], c=spectral_labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.title('Spectral Clustering using t-SNE')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
